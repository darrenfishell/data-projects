{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INITIALIZE EVERYTHING, FUNCTIONS, ETC.\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import config\n",
    "import datadotworld as dw\n",
    "import pygsheets\n",
    "import http.client\n",
    "import json\n",
    "import time\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "##Schedule_a API guide: https://api.open.fec.gov/developers/#/receipts/get_schedules_schedule_a_Ëœ\n",
    "\n",
    "##Define functions\n",
    "def get_cands(**kwargs):\n",
    "    state = kwargs.get('state')\n",
    "    cycle = kwargs.get('cycle')\n",
    "    url = 'https://api.open.fec.gov/v1/candidates/search'\n",
    "\n",
    "    params = {'election_year': cycle\n",
    "              , 'state': state\n",
    "              , 'api_key': config.fec_key\n",
    "              , 'is_active_candidate': True\n",
    "              , 'has_raised_funds': True\n",
    "              }\n",
    "\n",
    "    params = {k: v for k, v in params.items() if v}\n",
    "\n",
    "    def cand_req():\n",
    "\n",
    "        cand_all = []\n",
    "        r = requests.get(url, params=params).json()\n",
    "        page = r['pagination']['page']\n",
    "\n",
    "        while page <= r['pagination']['pages']:\n",
    "\n",
    "            cands = json_normalize(data=r['results'])[['candidate_id'\n",
    "                , 'name'\n",
    "                , 'party_full'\n",
    "                , 'incumbent_challenge_full'\n",
    "                , 'office_full'\n",
    "                , 'first_file_date'\n",
    "                                                       ]]\n",
    "            comm = json_normalize(data=r['results'], record_path='principal_committees')\n",
    "            comm = comm[['candidate_ids'\n",
    "                , 'committee_id'\n",
    "                , 'name']]\n",
    "\n",
    "            comm['candidate_ids'] = comm['candidate_ids'].str[0]\n",
    "\n",
    "            # Merge candidate and committee lookups\n",
    "            cands = cands.merge(comm, left_on='candidate_id', right_on='candidate_ids')\n",
    "\n",
    "            # Rename cols\n",
    "            colnm = {\n",
    "                'name_x': 'candidate_name'\n",
    "                , 'name_y': 'committee_name'\n",
    "            }\n",
    "\n",
    "            cands.rename(columns=colnm, inplace=True)\n",
    "            cands.drop(columns='candidate_ids', inplace=True)\n",
    "\n",
    "            cand_all.append(cands)\n",
    "\n",
    "            page+=1\n",
    "\n",
    "            params.update(page=page)\n",
    "\n",
    "            r = requests.get(url, params=params).json()\n",
    "\n",
    "        return cand_all\n",
    "\n",
    "    if cycle:\n",
    "\n",
    "        for state in state:\n",
    "\n",
    "            params['state'] = state\n",
    "\n",
    "            for cycle in cycle:\n",
    "\n",
    "                cand_all = cand_req()\n",
    "\n",
    "    else:\n",
    "\n",
    "        for state in state:\n",
    "\n",
    "            params['state'] = state\n",
    "\n",
    "            cand_all = cand_req()\n",
    "\n",
    "    cand_all = pd.concat(cand_all, sort=False, ignore_index=True).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    return cand_all\n",
    "\n",
    "def get_committees(names):\n",
    "    names = names.split(',')\n",
    "\n",
    "    comms = []\n",
    "    end = 'https://api.open.fec.gov/v1/names/committees'\n",
    "\n",
    "    for name in names:\n",
    "        params = {'q': name\n",
    "            , 'api_key': config.fec_key}\n",
    "\n",
    "        r = requests.get(end, params=params).json()\n",
    "\n",
    "        comm = json_normalize(data=r['results'])\n",
    "\n",
    "        comms.append(comm)\n",
    "\n",
    "    comm_all = pd.concat(comms, sort=False, ignore_index=True).drop_duplicates()\n",
    "\n",
    "    return comm_all\n",
    "\n",
    "\n",
    "def get_itemized(cycle, cands):\n",
    "\n",
    "    def get_unitem(cycle, commid):\n",
    "\n",
    "        end = 'https://api.open.fec.gov/v1/committee/'\n",
    "        params = {\n",
    "            'api_key': config.fec_key\n",
    "            , 'cycle': cycle\n",
    "            , 'per_page': '100'\n",
    "            , 'committee_id': commid\n",
    "        }\n",
    "\n",
    "        params = {k: v for k, v in params.items() if v}\n",
    "\n",
    "        # Collect unitemized contributions\n",
    "        r = requests.get(end + commid + '/totals', params=params).json()\n",
    "        udf = json_normalize(r['results'])\n",
    "        return udf\n",
    "\n",
    "    end = 'https://api.open.fec.gov/v1/schedules/schedule_a/'\n",
    "    ids = cands['committee_id']\n",
    "    dfs = []\n",
    "    udfs = []\n",
    "    page_count = 0\n",
    "    cand_count = len(cands)\n",
    "\n",
    "    for idx, commid in enumerate(ids):\n",
    "\n",
    "        for_start = time.time()\n",
    "        r_count = 0\n",
    "\n",
    "        item_page = 0\n",
    "\n",
    "        params = {\n",
    "            'per_page': '100'\n",
    "            , 'sort': 'contribution_receipt_date'\n",
    "            , 'api_key': config.fec_key\n",
    "            , 'is_individual': 'true'\n",
    "            , 'two_year_transaction_period': cycle\n",
    "            , 'last_index': []\n",
    "            , 'last_contribution_receipt_date': []\n",
    "            , 'committee_id': commid\n",
    "        }\n",
    "        params = {k: v for k, v in params.items() if v}\n",
    "\n",
    "        try:\n",
    "            udfs.append(get_unitem(cycle, commid))\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        # Initialize Schedule A request\n",
    "        r = requests.get(end, params=params).json()\n",
    "\n",
    "        try:\n",
    "            pages = r['pagination']['pages']\n",
    "        except:\n",
    "            pages=0\n",
    "\n",
    "        r_count += 1\n",
    "\n",
    "        candidate = cands['candidate_name'][idx]\n",
    "        print(f'Loading contributions for {candidate}')\n",
    "\n",
    "        try:\n",
    "            while r['pagination']['last_indexes'] is not None:\n",
    "                df = json_normalize(r['results'])\n",
    "                dfs.append(df)\n",
    "\n",
    "                last_index = r['pagination']['last_indexes']['last_index']\n",
    "                last_date = r['pagination']['last_indexes']['last_contribution_receipt_date']\n",
    "\n",
    "                params.update([('last_index', last_index)\n",
    "                              , ('last_contribution_receipt_date', last_date)])\n",
    "\n",
    "                r = requests.get(end, params=params).json()\n",
    "                r_count += 1\n",
    "                page_count += 1\n",
    "                item_page += 1\n",
    "\n",
    "                for_duration = time.time() - for_start\n",
    "                r_rate = r_count / for_duration\n",
    "                sleep = abs(r_rate - 1)\n",
    "\n",
    "                if r_rate >= 1:\n",
    "                    print(f'Hit rate {r_rate} on {candidate} page {page_count}')\n",
    "                    time.sleep(sleep)\n",
    "\n",
    "        except:\n",
    "            print(f'Broke on page {item_page} for {candidate}.')\n",
    "            print(f'Last index: {last_index} //n Last date: {last_date} //n commid: {commid}')\n",
    "            print(f'Reached page {item_page} of {pages} for {candidate}.')\n",
    "            time.sleep(sleep)\n",
    "\n",
    "    print(f'{page_count} pages for {cand_count} candidates')\n",
    "\n",
    "    # After for loop, concatenate dfs\n",
    "    df = pd.concat(dfs, sort=False, ignore_index=True).drop_duplicates(subset='transaction_id')\n",
    "    ustore = pd.concat(udfs, sort=False, ignore_index=True).drop_duplicates()\n",
    "\n",
    "    # Clean dataframe\n",
    "    df['contributor_zip'] = df['contributor_zip'].str[:5]\n",
    "\n",
    "    # Filter to is_individual and no memoed subtotal\n",
    "    df = df[(df['is_individual'] == True) | (df['memoed_subtotal'] == False)]\n",
    "\n",
    "    # Transform unitemized table, based on df structure\n",
    "    cols = df.columns.values.tolist()\n",
    "    udf = pd.DataFrame(columns=cols)\n",
    "\n",
    "    targetcols = ['committee.name'\n",
    "        , 'committee.party_full'\n",
    "        , 'committee_id'\n",
    "        , 'contribution_receipt_amount'\n",
    "        , 'contribution_receipt_date'\n",
    "        , 'fec_election_type_desc']\n",
    "\n",
    "    sourcecols = ['committee_name'\n",
    "        , 'party_full'\n",
    "        , 'committee_id'\n",
    "        , 'individual_unitemized_contributions'\n",
    "        , 'coverage_end_date'\n",
    "        , 'last_report_type_full']\n",
    "\n",
    "    udf[targetcols] = ustore[sourcecols]\n",
    "\n",
    "    # Add labels\n",
    "    udf['contributor_name'] = 'Unitemized individual contributions'\n",
    "    udf['entity_type'] = 'IND'\n",
    "\n",
    "    # Combine dataframes\n",
    "    df = pd.concat([df, udf], sort=False, ignore_index=True)\n",
    "\n",
    "    # Parse datetime\n",
    "    df['contribution_receipt_date'] = df['contribution_receipt_date'].str.split('T', expand=True)[0]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_ies(cycle, cands):\n",
    "\n",
    "    end = 'https://api.open.fec.gov/v1/schedules/schedule_e/'\n",
    "    ids = cands['candidate_id']\n",
    "    dfs = []\n",
    "    page_count = 0\n",
    "\n",
    "    for idx, item in enumerate(ids):\n",
    "\n",
    "        for_start = time.time()\n",
    "        r_count = 0\n",
    "\n",
    "        params = {\n",
    "            'per_page': '100'\n",
    "            , 'api_key': config.fec_key\n",
    "            , 'cycle': cycle\n",
    "            , 'last_index': []\n",
    "            , 'last_expenditure_date': []\n",
    "            , 'candidate_id': item\n",
    "        }\n",
    "\n",
    "        params = {k: v for k, v in params.items() if v}\n",
    "\n",
    "        r = requests.get(end, params=params).json()\n",
    "\n",
    "        candidate = cands['candidate_name'][idx]\n",
    "\n",
    "        print(f'Loading IEs for {candidate}')\n",
    "\n",
    "        try:\n",
    "            pages = str(r['pagination']['pages'])\n",
    "        except:\n",
    "            pages = 0\n",
    "\n",
    "        if pages == 0:\n",
    "            try:\n",
    "                df = json_normalize(r['results'])\n",
    "                dfs.append(df)\n",
    "            except:\n",
    "                df = []\n",
    "\n",
    "        else:\n",
    "            while r['pagination']['last_indexes'] is not None:\n",
    "                df = json_normalize(r['results'])\n",
    "                dfs.append(df)\n",
    "\n",
    "                last_index = r['pagination']['last_indexes']['last_index']\n",
    "                last_date = r['pagination']['last_indexes']['last_expenditure_date']\n",
    "\n",
    "                params.update([('last_index', last_index)\n",
    "                                  , ('last_expenditure_date', last_date)])\n",
    "\n",
    "                r = requests.get(end, params=params).json()\n",
    "                r_count += 1\n",
    "                page_count += 1\n",
    "\n",
    "                for_duration = time.time() - for_start\n",
    "                r_rate = r_count / for_duration\n",
    "                sleep = abs(r_rate - 1)\n",
    "\n",
    "                if r_rate >= 1:\n",
    "                    print(f'Hit rate {r_rate} on {candidate} page {page_count}')\n",
    "                    time.sleep(1)\n",
    "\n",
    "    # After for loop, concatenate dfs\n",
    "    df = pd.concat(dfs, sort=False, ignore_index=True).drop_duplicates(subset='transaction_id')\n",
    "\n",
    "    # Clean dataframe\n",
    "    df['commiteee.zip'] = df['committee.zip'].str[:5]\n",
    "    df['expenditure_date'] = df['expenditure_date'].str.split('T', expand=True)[0]\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_coordinated(cycle, cands):\n",
    "\n",
    "    end = 'https://api.open.fec.gov/v1/schedules/schedule_f/'\n",
    "    ids = cands['candidate_id']\n",
    "    dfs = []\n",
    "    page_count = 0\n",
    "\n",
    "    for i, item in enumerate(ids):\n",
    "\n",
    "        params = {\n",
    "            'per_page': '100'\n",
    "            , 'api_key': config.fec_key\n",
    "            , 'two_year_transaction_period': cycle\n",
    "            , 'page': 1\n",
    "            , 'candidate_id': item\n",
    "        }\n",
    "        params = {k: v for k, v in params.items() if v}\n",
    "\n",
    "        r = requests.get(end, params=params).json()\n",
    "\n",
    "        cur_page = r['pagination']['page']\n",
    "        all_pgs = r['pagination']['pages']\n",
    "\n",
    "        candidate = cands['candidate_name'][i]\n",
    "\n",
    "        for page in range(all_pgs):\n",
    "            for_start = time.time()\n",
    "            r_count = 0\n",
    "\n",
    "            params.update([('page', page + 1)])\n",
    "            r = requests.get(end, params=params).json()\n",
    "\n",
    "            r_count += 1\n",
    "            page_count += 1\n",
    "\n",
    "            for_duration = time.time() - for_start\n",
    "            r_rate = r_count / for_duration\n",
    "            sleep = abs(r_rate - 1)\n",
    "\n",
    "            if r_rate >= 1:\n",
    "                print(f'Hit rate {r_rate} on {candidate} page {page_count}')\n",
    "                time.sleep(sleep)\n",
    "\n",
    "            df = json_normalize(r['results'])\n",
    "            dfs.append(df)\n",
    "\n",
    "    # After for loop, concatenate dfs\n",
    "    df = pd.concat(dfs, sort=False, ignore_index=True).drop_duplicates(subset='transaction_id')\n",
    "\n",
    "    # Clean dataframe\n",
    "    df['commiteee.zip'] = df['committee.zip'].str[:5]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_summary(cycle, cands):\n",
    "\n",
    "    end = 'https://api.open.fec.gov/v1/committee/'\n",
    "    ids = cands['committee_id']\n",
    "    dfs = []\n",
    "\n",
    "    for idx, id in enumerate(ids):\n",
    "        params = {\n",
    "            'api_key': config.fec_key\n",
    "            , 'cycle': cycle\n",
    "            , 'per_page': '100'\n",
    "        }\n",
    "        params = {k: v for k, v in params.items() if v}\n",
    "\n",
    "        r = requests.get(end + id + '/totals', params=params).json()\n",
    "        try:\n",
    "            df = json_normalize(r['results'])\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        dfs.append(df)\n",
    "\n",
    "    # After for loop, concatenate dfs\n",
    "    df = pd.concat(dfs, sort=False, ignore_index=True).drop_duplicates()\n",
    "\n",
    "    return df\n",
    "\n",
    "def write_cands(df):\n",
    "    results = dw.query('darrenfishell/2020-election-repo'\n",
    "                       , 'SELECT * FROM candidate_committee_lookup').dataframe\n",
    "\n",
    "    merged_df = pd.concat([results, df]).drop_duplicates(subset=['committee_id']).reset_index(drop=True)\n",
    "\n",
    "    oldlen = len(results)\n",
    "    newlen = len(merged_df)\n",
    "\n",
    "    with dw.open_remote_file('darrenfishell/2020-election-repo', 'candidate_committee_lookup.csv') as w:\n",
    "        merged_df.to_csv(w, index=False)\n",
    "\n",
    "    return oldlen, newlen\n",
    "\n",
    "def write_indiv(df):\n",
    "    results = dw.query('darrenfishell/2020-election-repo'\n",
    "                       , 'SELECT * FROM individual_congressional_contributions').dataframe\n",
    "\n",
    "    subset = ['contribution_receipt_date', 'committee_id', 'transaction_id']\n",
    "\n",
    "    merged_df = pd.concat([results, df], sort=False, ignore_index=True).drop_duplicates(subset=subset)\n",
    "\n",
    "    oldlen = len(results)\n",
    "    newlen = len(merged_df)\n",
    "\n",
    "    test = oldlen < newlen\n",
    "\n",
    "    if test:\n",
    "        with dw.open_remote_file('darrenfishell/2020-election-repo', 'individual-congressional-contributions.csv') as w:\n",
    "            merged_df.to_csv(w, index=False)\n",
    "    return test, oldlen, newlen\n",
    "\n",
    "\n",
    "def write_summary(df):\n",
    "    results = dw.query('darrenfishell/2020-election-repo'\n",
    "                       , 'SELECT * FROM congress_financial_summaries').dataframe\n",
    "\n",
    "    merged_df = pd.concat([df, results], sort=False, ignore_index=True).drop_duplicates()\n",
    "\n",
    "    oldcash = sum(results['receipts'])\n",
    "    oldlen = len(results)\n",
    "    newcash = sum(merged_df['receipts'])\n",
    "    newlen = len(merged_df)\n",
    "\n",
    "    test = oldcash < newcash\n",
    "\n",
    "    if test:\n",
    "        with dw.open_remote_file('darrenfishell/2020-election-repo', 'congress_financial_summaries.csv') as w:\n",
    "            merged_df.to_csv(w, index=False)\n",
    "    return test, oldlen, newlen\n",
    "\n",
    "\n",
    "def write_ies(df):\n",
    "    results = dw.query('darrenfishell/2020-election-repo'\n",
    "                       , 'SELECT * FROM congress_independent_expenditures').dataframe\n",
    "\n",
    "    subset = ['transaction_id', 'candidate_id', 'filing_date']\n",
    "\n",
    "    merged_df = pd.concat([df, results], sort=False, ignore_index=True).drop_duplicates(subset=subset)\n",
    "\n",
    "    oldlen = len(results)\n",
    "    newlen = len(merged_df)\n",
    "\n",
    "    test = oldlen < newlen\n",
    "\n",
    "    if test:\n",
    "        with dw.open_remote_file('darrenfishell/2020-election-repo', 'congress-independent-expenditures.csv') as w:\n",
    "            merged_df.to_csv(w, index=False)\n",
    "    return test, oldlen, newlen\n",
    "\n",
    "\n",
    "def write_coord(df):\n",
    "    results = dw.query('darrenfishell/2020-election-repo'\n",
    "                       , 'SELECT * FROM congress_party_coordinated_expenditures').dataframe\n",
    "\n",
    "    subset = ['transaction_id', 'candidate_id', 'expenditure_date']\n",
    "\n",
    "    merged_df = pd.concat([df, results], sort=False, ignore_index=True).drop_duplicates(subset=subset)\n",
    "\n",
    "    oldlen = len(results)\n",
    "    newlen = len(merged_df)\n",
    "\n",
    "    test = oldlen < newlen\n",
    "\n",
    "    if test:\n",
    "        with dw.open_remote_file('darrenfishell/2020-election-repo',\n",
    "                                 'congress-party-coordinated-expenditures.csv') as w:\n",
    "            merged_df.to_csv(w, index=False)\n",
    "    return test, oldlen, newlen\n",
    "\n",
    "\n",
    "def write_to_gsheet():\n",
    "    gc = pygsheets.authorize(service_file='gcreds.json')\n",
    "    conn = http.client.HTTPSConnection(\"api.data.world\")\n",
    "    headers = {'authorization': \"Bearer \" + config.dw_key}\n",
    "\n",
    "    sheets_to_dw = [['maine-congress-2020', 'e2b1bde2-1e60-4d49-bd31-da5aa7ce0611', 1],\n",
    "                    ['maine-congress-2020', '026e8f40-d10e-4324-8b45-80dbc0e61627', 0]]\n",
    "\n",
    "    for idx, sheet in enumerate(sheets_to_dw):\n",
    "        sheet = [x[0] for x in sheets_to_dw][idx]\n",
    "        queryid = [x[1] for x in sheets_to_dw][idx]\n",
    "        gsh_idx = [x[2] for x in sheets_to_dw][idx]\n",
    "\n",
    "        # Retrieve query\n",
    "        conn.request(\"GET\", \"/v0/queries/\" + queryid, headers=headers)\n",
    "        data = conn.getresponse().read()\n",
    "        # Execute Query\n",
    "        results = dw.query('darrenfishell/2020-election-repo', json.loads(data)['body']).dataframe\n",
    "\n",
    "        # Prepare to load into Google Sheets\n",
    "        sh = gc.open(sheet)\n",
    "        wks = sh.worksheet('index', gsh_idx)\n",
    "        wks.clear()\n",
    "        wks.rows = results.shape[0]\n",
    "        wks.set_dataframe(results, start='A1', nan='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate lookup query failed.\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'cands' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-530a4072acd3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m \u001b[0mlambda_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_event\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-530a4072acd3>\u001b[0m in \u001b[0;36mlambda_handler\u001b[0;34m(event, context)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# Filename - input pairs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     files_input = {\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0;34m'itemized contributions'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcycle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcands\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;34m'campaign summary'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcycle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcands\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;34m'independent expenditures'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcycle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcands\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'cands' referenced before assignment"
     ]
    }
   ],
   "source": [
    "##MAIN.PY\n",
    "import time\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    start = time.time()\n",
    "    # Step 1: Set state(s) and cycle(s) for candidate search\n",
    "    state = ['ME']\n",
    "    cycle = ['2020']\n",
    "\n",
    "    # Query candidates and write to data.world\n",
    "    try:\n",
    "        cands = get_cands(state=state, cycle=cycle)\n",
    "        oldlen, newlen = write_cands(cands)\n",
    "\n",
    "        print(f'Wrote {newlen} candidate records. Prior load had {oldlen} records.')\n",
    "\n",
    "    except:\n",
    "        print('Candidate lookup query failed.')\n",
    "\n",
    "    # Get - write contribution pairs\n",
    "    getwrite = {\n",
    "        get_itemized: write_indiv,\n",
    "        get_summary: write_summary,\n",
    "        get_ies: write_ies,\n",
    "        get_coordinated: write_coord\n",
    "    }\n",
    "\n",
    "    # Filename - input pairs\n",
    "    files_input = {\n",
    "        'itemized contributions': [cycle, cands],\n",
    "        'campaign summary': [cycle, cands],\n",
    "        'independent expenditures': [cycle, cands],\n",
    "        'party coordinated expenditures': [cycle, cands]\n",
    "    }\n",
    "\n",
    "    # List of functions, filenames and inputs to unpack\n",
    "    files = [x[0] for x in list(files_input.items())]\n",
    "    params = [x[1] for x in list(files_input.items())]\n",
    "\n",
    "    # Iterate over all get-write functions, with TRY\n",
    "    for idx, (get, write) in enumerate(getwrite.items()):\n",
    "\n",
    "        # Set filename\n",
    "        file = files[idx]\n",
    "\n",
    "        print(f'getwrite index: {idx}')\n",
    "        # Run function r, return dataframes\n",
    "        df = get(*params[idx])\n",
    "\n",
    "        # Execute write functions to write to datadotworld\n",
    "        newtest, oldlen, newlen = write(df)\n",
    "        newrecords = newlen\n",
    "\n",
    "        if newtest:\n",
    "            print(f'Wrote {newrecords} new records to {file}, which had {oldlen}.')\n",
    "        else:\n",
    "            print(f'No update to {file}, which has {newlen} records.')\n",
    "\n",
    "    end = time.time()\n",
    "    duration = end - start\n",
    "    print(f'Script ran for {duration}')\n",
    "\n",
    "#     try:\n",
    "#         write_to_gsheet()\n",
    "#         print('Wrote to GSheets')\n",
    "#     except:\n",
    "#         print('Failed to write to GSheet')\n",
    "\n",
    "##TESTING\n",
    "test_event = {\n",
    "  \"key1\": \"value1\",\n",
    "  \"key2\": \"value2\",\n",
    "  \"key3\": \"value3\"\n",
    "}\n",
    "event = []\n",
    "\n",
    "lambda_handler(event, test_event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225584\n",
      "137005\n"
     ]
    }
   ],
   "source": [
    "results = dw.query('darrenfishell/2020-election-repo'\n",
    "                       , 'SELECT * FROM individual_congressional_contributions').dataframe\n",
    "\n",
    "subset = ['contribution_receipt_date', 'committee_id', 'transaction_id']\n",
    "\n",
    "print(len(results))\n",
    "\n",
    "results.drop_duplicates(subset=subset, inplace=True)\n",
    "\n",
    "print(len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136754\n"
     ]
    }
   ],
   "source": [
    "print(len(results.drop_duplicates(subset=subset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-464116190a57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mdw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_remote_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'darrenfishell/2020-election-repo'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'individual-congressional-contributions.csv'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mr_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/fec-elt/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal)\u001b[0m\n\u001b[1;32m   3226\u001b[0m             \u001b[0mdecimal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecimal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3227\u001b[0m         )\n\u001b[0;32m-> 3228\u001b[0;31m         \u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3230\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fec-elt/lib/python3.6/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    200\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnicodeWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mwriter_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fec-elt/lib/python3.6/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_save_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fec-elt/lib/python3.6/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36m_save_chunk\u001b[0;34m(self, start_i, end_i)\u001b[0m\n\u001b[1;32m    354\u001b[0m         )\n\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m         \u001b[0mlibwriters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_csv_rows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mpandas/_libs/writers.pyx\u001b[0m in \u001b[0;36mpandas._libs.writers.write_csv_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fec-elt/lib/python3.6/site-packages/datadotworld/files.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \"\"\"\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mode\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fec-elt/lib/python3.6/queue.py\u001b[0m in \u001b[0;36mput\u001b[0;34m(self, item, block, timeout)\u001b[0m\n\u001b[1;32m    131\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                     \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxsize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fec-elt/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with dw.open_remote_file('darrenfishell/2020-election-repo', 'individual-congressional-contributions.csv') as w:\n",
    "    r_df.to_csv(w, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = dw.query('darrenfishell/2020-election-repo'\n",
    "                       , 'SELECT * FROM candidate_committee_lookup').dataframe\n",
    "\n",
    "with dw.open_remote_file('darrenfishell/maine-federal-campaign-finance-tables', 'candidate_committee_lookup.csv') as w:\n",
    "    results.to_csv(w, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote to GSheets\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    write_to_gsheet()\n",
    "    print('Wrote to GSheets')\n",
    "except:\n",
    "    print('Failed to write to GSheet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAINELY IN BUSINESS ###\n",
    "\n",
    "The following scripts scrape the Maine Secretary of State's website for businesses with \"Mainely\" in the title. This happens in two steps. \n",
    "\n",
    "Because the SoS site limits searches to 100 results, the script first generates the full list of such businesses by searching for the combination of Mainely + each letter of the alphabet, combining that into one list that includes URLs for each individual business page.\n",
    "\n",
    "The second portion of the script scrapes business information from those individual business URLs and pulls that into the same dataframe, for output to CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import string\n",
    "import urllib\n",
    "import datadotworld\n",
    "import os\n",
    "from scrapy import Selector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pull down Mainely business names\n",
    "\n",
    "This loop uses a POST method to generate a list of businesses with \"Mainely\" in the title, from the Maine SoS website. It collects the tables and individual URLs for each page of results. \n",
    "\n",
    "The URLs serve as unique identifiers for records and are used to drop any duplicate records. They are then used in the next step, to pull in additional information about each business."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of uppercase letters for loop\n",
    "alpha = string.ascii_uppercase\n",
    "\n",
    "# Mainely loop and variables\n",
    "id=0\n",
    "q = 'Mainely '+alpha[id]\n",
    "url = 'https://icrs.informe.org/nei-sos-icrs/ICRS?MainPage=x'\n",
    "url_base = 'https://icrs.informe.org'\n",
    "\n",
    "#ID and variable to loop through alphabet\n",
    "data = {'WAISqueryString':q\n",
    "       ,'number':''\n",
    "       ,'search': {\n",
    "           '0':'Click+Here+to+Search'\n",
    "           ,'1':'search'\n",
    "       }}\n",
    "\n",
    "#POST headers\n",
    "headers = {'Host':'icrs.informe.org'\n",
    "            ,'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.14; rv:69.0) Gecko/20100101 Firefox/69.0'\n",
    "            ,'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'\n",
    "            ,'Accept-Language': 'en-US,en;q=0.5'\n",
    "            ,'Accept-Encoding': 'gzip, deflate, br'\n",
    "            ,'Content-Type': 'application/x-www-form-urlencoded'\n",
    "            ,'Connection': 'keep-alive'\n",
    "            ,'Cookie': 'JSESSIONID=0DF53489E916020D19FCCAB79D9255EB'\n",
    "            ,'Referer': 'https://icrs.informe.org/nei-sos-icrs/ICRS?search=&MainPage=x&newsearch=New+Search'\n",
    "            ,'Upgrade-Insecure-Requests': '1'}\n",
    "\n",
    "### MAINELY NAMES LOOP ###\n",
    "dfs=[]\n",
    "\n",
    "#Loop limited to [alphabet length]-1, which gets to Z, at index 25\n",
    "for x in range(0,len(alpha)-1):\n",
    "    \n",
    "    #Pull in request URL text\n",
    "    r = requests.post(url, data=data, headers=headers)\n",
    "    \n",
    "    #Make Selector item to scrape\n",
    "    sel = Selector(text = r.text)\n",
    "    \n",
    "    #Scrape Names, Type & URL and merge\n",
    "    names = sel.xpath('//tr[position()>=6]/td[2]//text()').extract()\n",
    "    type = sel.xpath('//tr[position()>=6]/td[3]//text()').extract()\n",
    "    \n",
    "    ##URL handler\n",
    "    rel_urls = sel.xpath('//tr[position()>=6]/td[4]//a/@href').extract()\n",
    "    n = 0\n",
    "    full_urls=[]\n",
    "    for x in rel_urls:\n",
    "        full_urls.append(url_base + rel_urls[n])\n",
    "        n += 1\n",
    "    \n",
    "    #Concatenate all lists to dataframe\n",
    "    df = pd.DataFrame({'names':names\n",
    "                      ,'type':type\n",
    "                      ,'urls':full_urls\n",
    "                      })\n",
    "    dfs.append(df)\n",
    "    id+=1\n",
    "    q = 'Mainely '+alpha[id]\n",
    "    data.update(WAISqueryString=q)\n",
    "\n",
    "#Combine DF results, reset DF index, drop duplicate rows by URL only\n",
    "mainely_biz=pd.concat(dfs,sort=False,ignore_index=True)\n",
    "mainely_biz=mainely_biz.drop_duplicates(subset='urls').reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pull in Mainely business details\n",
    "\n",
    "Using the URLs from the prior step, these operations pull in new details from the individual business registry pages, including filing dates and registered agents.\n",
    "\n",
    "All of these lists are then concatenated with the original list into a new dataframe that is ready for cleaning steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HARVEST INDIVIDUAL BUSINESS DETAILS\n",
    "\n",
    "#Initialize lists to hold scraped variables\n",
    "status=[]\n",
    "org_type=[]\n",
    "address=[]\n",
    "filing_date=[]\n",
    "owner_clerk=[]\n",
    "\n",
    "#Index\n",
    "i=int(0)\n",
    "\n",
    "for x in mainely_biz['urls']:\n",
    "\n",
    "    sel = Selector(text = requests.get(mainely_biz['urls'][i]).content)\n",
    "    \n",
    "    if mainely_biz['type'][i] == 'MARK':\n",
    "        status.append(sel.xpath('//table//b[contains(text(),\"Status\")]//ancestor::tr[1]/following::tr[1]/td[2]/text()').get())\n",
    "        org_type.append(sel.xpath('//table//b[contains(text(),\"Owner Type\")]//ancestor::tr[1]/following::tr[1]/td[5]/text()').get())\n",
    "        address.append(sel.xpath('//table//b[contains(text(),\"Owner\")]//ancestor::tr[1]/following::tr[3]/td/text()[last()-1] | //table//b[contains(text(),\"Owner\")]//ancestor::tr[1]/following::tr[3]/td/text()[last()]').extract())\n",
    "        owner_clerk.append(sel.xpath('//table//b[contains(text(),\"Owner\")]//ancestor::tr[1]/following::tr[3]/td/text()[1]').extract())\n",
    "        filing_date.append(sel.xpath('//table//b[contains(text(),\"Filing Date\")]//ancestor::tr[1]/following::tr[1]/td[2]/text()').extract())\n",
    "    else: \n",
    "        status.append(sel.xpath('//table//b[contains(text(),\"Status\")]//ancestor::tr[1]/following::tr[1]/td[4]').get())\n",
    "        org_type.append(sel.xpath('//table//b[contains(text(),\"Filing Type\")]//ancestor::tr[1]/following::tr[1]/td[3]').get())\n",
    "        filing_date.append(sel.xpath('//table//b[contains(text(),\"Filing Date\")]//ancestor::tr[1]/following::td[1]/text()').extract())\n",
    "        \n",
    "        if mainely_biz['type'][i] == 'RESERVED':\n",
    "            address.append(sel.xpath('//table//b[contains(text(),\"Contact\")]//ancestor::tr[1]/following::tr[1]/td/text()[following::br][position()>(last()-(last()-1))]').extract())\n",
    "            owner_clerk.append(sel.xpath('//table//b[contains(text(),\"Contact\")]//ancestor::tr[1]/following::tr[1]/td/text()[following::br][1]').extract())\n",
    "        else:\n",
    "            address.append(sel.xpath('//table//b[contains(text(),\"Clerk\")]//ancestor::tr[1]/following::tr[1]/td/text()[following::br][position()>(last()-(last()-1))]').extract())\n",
    "            owner_clerk.append(sel.xpath('//table//b[contains(text(),\"Clerk\")]//ancestor::tr[1]/following::tr[1]/td/text()[following::br][1]').extract())\n",
    "    i+=1\n",
    "    \n",
    "#COMBINE PARENT DATA AND BUSINESS DETAILS\n",
    "big_df = pd.concat([mainely_biz,pd.DataFrame({'status':status\n",
    "                                      ,'owner/org_type':org_type\n",
    "                                      ,'address':address\n",
    "                                      ,'owner_or_clerk':owner_clerk\n",
    "                                      ,'filing_date':filing_date})], axis=1)\n",
    "big_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_df[big_df['type']=='RESERVED']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning\n",
    "\n",
    "List fields are converted to strings, preparing them for trimming and replacement of unneccessary characters. The script previews the dataframe again, to compare with the output from the previous step, before cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##DATA/STRING CLEANING\n",
    "\n",
    "#Convert lists to strings\n",
    "big_df[['address'\n",
    "        ,'owner_or_clerk'\n",
    "        ,'filing_date']] = big_df[['address'\n",
    "                                   ,'owner_or_clerk'\n",
    "                                   ,'filing_date']].astype(str)\n",
    "\n",
    "#Eliminate <td> tags\n",
    "replace_dict = {'<td>':''\n",
    "               ,'</td>':''\n",
    "               ,r'\\[|\\]':''\n",
    "               ,r'\\\\n|\\\\t':''\n",
    "               ,r\"\\'\":''\n",
    "               }\n",
    "\n",
    "big_df.replace(replace_dict,regex=True,inplace=True)\n",
    "\n",
    "#Cleaning HTML out of data\n",
    "def trim_all_columns(df):\n",
    "    \"\"\"\n",
    "    Trim whitespace from all series in dataframe\n",
    "    \"\"\"\n",
    "    trim_strings = lambda x: x.strip() if isinstance(x, str) else x\n",
    "    return df.applymap(trim_strings)\n",
    "\n",
    "big_df = trim_all_columns(big_df)\n",
    "\n",
    "#PREVIEW DATAFRAME\n",
    "big_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OUTPUT TO CSV\n",
    "cwd = os.getcwd()\n",
    "big_df.to_csv('mainely_businesses_scraped.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## WRITE TO DATA.WORLD ##\n",
    "with dw.open_remote_file('darrenfishell/mainely-businesses', 'mainely-business-names.csv') as w:\n",
    "    big_df.to_csv(w, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

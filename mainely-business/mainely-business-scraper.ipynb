{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAINELY IN BUSINESS ###\n",
    "\n",
    "The following scripts scrape the Maine Secretary of State's website for businesses with \"Mainely\" in the title. This happens in two steps. \n",
    "\n",
    "Because the SoS site limits searches to 100 results, the script first generates the full list of such businesses by searching for the combination of Mainely + each letter of the alphabet, combining that into one list that includes URLs for each individual business page.\n",
    "\n",
    "The second portion of the script scrapes business information from those individual business URLs and pulls that into the same dataframe, for output to CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import string\n",
    "import time\n",
    "import datadotworld as dw\n",
    "import os\n",
    "import random\n",
    "from scrapy import Selector\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pull down Mainely business names\n",
    "\n",
    "This loop uses a POST method to generate a list of businesses with \"Mainely\" in the title, from the Maine SoS website. It collects the tables and individual URLs for each page of results. \n",
    "\n",
    "The URLs serve as unique identifiers for records and are used to drop any duplicate records. They are then used in the next step, to pull in additional information about each business."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of uppercase letters and numbers for search loop\n",
    "alpha = string.ascii_uppercase + string.digits\n",
    "\n",
    "# Mainely loop and variables\n",
    "id=0\n",
    "q = 'Mainely '+alpha[id]\n",
    "url = 'https://icrs.informe.org/nei-sos-icrs/ICRS?MainPage=x'\n",
    "url_base = 'https://icrs.informe.org'\n",
    "\n",
    "#ID and variable to loop through alphabet\n",
    "data = {'WAISqueryString':q\n",
    "       ,'number':''\n",
    "       ,'search': {\n",
    "           '0':'Click+Here+to+Search'\n",
    "           ,'1':'search'\n",
    "       }}\n",
    "\n",
    "#POST headers\n",
    "headers = {'Host':'icrs.informe.org'\n",
    "            ,'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.14; rv:69.0) Gecko/20100101 Firefox/69.0'\n",
    "            ,'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'\n",
    "            ,'Accept-Language': 'en-US,en;q=0.5'\n",
    "            ,'Accept-Encoding': 'gzip, deflate, br'\n",
    "            ,'Content-Type': 'application/x-www-form-urlencoded'\n",
    "            ,'Connection': 'keep-alive'\n",
    "            ,'Cookie': 'JSESSIONID=0DF53489E916020D19FCCAB79D9255EB'\n",
    "            ,'Referer': 'https://icrs.informe.org/nei-sos-icrs/ICRS?search=&MainPage=x&newsearch=New+Search'\n",
    "            ,'Upgrade-Insecure-Requests': '1'}\n",
    "\n",
    "### MAINELY NAMES LOOP ###\n",
    "dfs=[]\n",
    "\n",
    "#Loops through alphabet and digits 0-9\n",
    "for x in range(0,len(alpha)-1):\n",
    "    \n",
    "    #Pull in request URL text\n",
    "    r = requests.post(url, data=data, headers=headers)\n",
    "    \n",
    "    #Make Selector item to scrape\n",
    "    sel = Selector(text = r.text)\n",
    "    \n",
    "    #Scrape Names, Type & URL and merge\n",
    "    names = sel.xpath('//tr[position()>=6]/td[2]//text()').extract()\n",
    "    type = sel.xpath('//tr[position()>=6]/td[3]//text()').extract()\n",
    "    \n",
    "    ##URL handler\n",
    "    rel_urls = sel.xpath('//tr[position()>=6]/td[4]//a/@href').extract()\n",
    "    n = 0\n",
    "    full_urls=[]\n",
    "    for x in rel_urls:\n",
    "        full_urls.append(url_base + rel_urls[n])\n",
    "        n += 1\n",
    "    \n",
    "    #Concatenate all lists to dataframe\n",
    "    df = pd.DataFrame({'names':names\n",
    "                      ,'type':type\n",
    "                      ,'urls':full_urls\n",
    "                      })\n",
    "    dfs.append(df)\n",
    "    id+=1\n",
    "    q = 'Mainely '+alpha[id]\n",
    "    data.update(WAISqueryString=q)\n",
    "\n",
    "#Combine DF results, reset DF index, drop duplicate rows by URL only\n",
    "mainely_biz=pd.concat(dfs,sort=False,ignore_index=True)\n",
    "mainely_biz=mainely_biz.drop_duplicates(subset='urls').reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Write initial scrape to disk, to enable testing\n",
    "# today = date.today().strftime(\"%d-%m-%Y\")\n",
    "# mainely_biz.to_csv('mainely-biz-scrape-'+today+'.csv')\n",
    "mainely_biz = pd.read_csv('mainely-biz-scrape-01-01-2020.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pull in Mainely business details\n",
    "\n",
    "Using the URLs from the prior step, these operations pull in new details from the individual business registry pages, including filing dates and registered agents.\n",
    "\n",
    "All of these lists are then concatenated with the original list into a new dataframe that is ready for cleaning steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delay: 116 seconds  index: 0\n",
      "delay: 293 seconds  index: 1\n",
      "delay: 71 seconds  index: 2\n",
      "delay: 35 seconds  index: 3\n",
      "delay: 292 seconds  index: 4\n",
      "delay: 94 seconds  index: 5\n",
      "delay: 200 seconds  index: 6\n",
      "delay: 177 seconds  index: 7\n",
      "delay: 146 seconds  index: 8\n",
      "delay: 119 seconds  index: 9\n",
      "delay: 234 seconds  index: 10\n",
      "delay: 194 seconds  index: 11\n",
      "delay: 51 seconds  index: 12\n",
      "delay: 51 seconds  index: 13\n",
      "delay: 174 seconds  index: 14\n",
      "delay: 30 seconds  index: 15\n",
      "delay: 27 seconds  index: 16\n",
      "delay: 66 seconds  index: 17\n",
      "delay: 205 seconds  index: 18\n",
      "delay: 135 seconds  index: 19\n",
      "delay: 96 seconds  index: 20\n",
      "delay: 19 seconds  index: 21\n",
      "delay: 236 seconds  index: 22\n",
      "delay: 203 seconds  index: 23\n",
      "delay: 104 seconds  index: 24\n",
      "delay: 116 seconds  index: 25\n",
      "delay: 169 seconds  index: 26\n",
      "delay: 97 seconds  index: 27\n",
      "delay: 53 seconds  index: 28\n"
     ]
    }
   ],
   "source": [
    "#HARVEST INDIVIDUAL BUSINESS DETAILS\n",
    "#Initialize lists to hold scraped variables\n",
    "status=[]\n",
    "org_type=[]\n",
    "address=[]\n",
    "filing_date=[]\n",
    "owner_clerk=[]\n",
    "\n",
    "\n",
    "#New headers\n",
    "headers = {'Host':'icrs.informe.org'\n",
    "            ,'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:71.0) Gecko/20100101 Firefox/71.0'\n",
    "            ,'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'\n",
    "            ,'Accept-Language': 'en-US,en;q=0.5'\n",
    "            ,'Accept-Encoding': 'gzip, deflate, br'\n",
    "            ,'Content-Type': 'application/x-www-form-urlencoded'\n",
    "            ,'Connection': 'keep-alive'\n",
    "            ,'DNT':'1'\n",
    "            ,'Connection':'keep-alive'\n",
    "            ,'Cookie':'JSESSIONID=5DB3513D307D954878674950FF081499'\n",
    "            ,'Upgrade-Insecure-Requests':'1'\n",
    "            ,'Cache-Control':'max-age=0'\n",
    "            ,'Referer': 'https://icrs.informe.org/nei-sos-icrs/ICRS'\n",
    "            ,'Upgrade-Insecure-Requests': '1'}\n",
    "\n",
    "\n",
    "# Host: icrs.informe.org\n",
    "# User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:71.0) Gecko/20100101 Firefox/71.0\n",
    "# Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\n",
    "# Accept-Language: en-US,en;q=0.5\n",
    "# Accept-Encoding: gzip, deflate, br\n",
    "# Referer: https://icrs.informe.org/nei-sos-icrs/ICRS\n",
    "# DNT: 1\n",
    "# Connection: keep-alive\n",
    "# Cookie: JSESSIONID=5DB3513D307D954878674950FF081499\n",
    "# Upgrade-Insecure-Requests: 1\n",
    "# Cache-Control: max-age=0\n",
    "\n",
    "\n",
    "#Index\n",
    "i=int(0)\n",
    "\n",
    "for i in range(0,len(mainely_biz)-1):\n",
    "    \n",
    "    #Set variable time delay for scrape\n",
    "    delay = random.randint(15,300)\n",
    "    print('delay: ' + str(delay) + ' seconds' + '  index: ' + str(i))\n",
    "    \n",
    "    try:\n",
    "        sel = Selector(text = requests.get(mainely_biz['urls'][i],headers=headers).content)\n",
    "    except:\n",
    "        print('URL ERROR on ' + mainely_biz['urls'][i])\n",
    "        break\n",
    "    \n",
    "    if mainely_biz['type'][i] == 'MARK':\n",
    "        status.append(sel.xpath('//table//b[contains(text(),\"Status\")]//ancestor::tr[1]/following::tr[1]/td[2]/text()').get())\n",
    "        org_type.append(sel.xpath('//table//b[contains(text(),\"Owner Type\")]//ancestor::tr[1]/following::tr[1]/td[5]/text()').get())\n",
    "        address.append(sel.xpath('//table//b[contains(text(),\"Owner\")]//ancestor::tr[1]/following::tr[3]/td/text()[last()-1] | //table//b[contains(text(),\"Owner\")]//ancestor::tr[1]/following::tr[3]/td/text()[last()]').extract())\n",
    "        owner_clerk.append(sel.xpath('//table//b[contains(text(),\"Owner\")]//ancestor::tr[1]/following::tr[3]/td/text()[1]').extract())\n",
    "        filing_date.append(sel.xpath('//table//b[contains(text(),\"Filing Date\")]//ancestor::tr[1]/following::tr[1]/td[2]/text()').extract())\n",
    "    else: \n",
    "        status.append(sel.xpath('//table//b[contains(text(),\"Status\")]//ancestor::tr[1]/following::tr[1]/td[4]').get())\n",
    "        org_type.append(sel.xpath('//table//b[contains(text(),\"Filing Type\")]//ancestor::tr[1]/following::tr[1]/td[3]').get())\n",
    "        filing_date.append(sel.xpath('//table//b[contains(text(),\"Filing Date\")]//ancestor::tr[1]/following::td[1]/text()').extract())\n",
    "        \n",
    "        if mainely_biz['type'][i] == 'RESERVED':\n",
    "            address.append(sel.xpath('//table//b[contains(text(),\"Contact\")]//ancestor::tr[1]/following::tr[1]/td/text()[following::br][position()>(last()-(last()-1))]').extract())\n",
    "            owner_clerk.append(sel.xpath('//table//b[contains(text(),\"Contact\")]//ancestor::tr[1]/following::tr[1]/td/text()[following::br][1]').extract())\n",
    "        else:\n",
    "            address.append(sel.xpath('//table//b[contains(text(),\"Clerk\")]//ancestor::tr[1]/following::tr[1]/td/text()[following::br][position()>(last()-(last()-1))]').extract())\n",
    "            owner_clerk.append(sel.xpath('//table//b[contains(text(),\"Clerk\")]//ancestor::tr[1]/following::tr[1]/td/text()[following::br][1]').extract())\n",
    "    i+=1\n",
    "    # Big sleep between requests\n",
    "    time.sleep(delay)\n",
    "    \n",
    "#COMBINE PARENT DATA AND BUSINESS DETAILS\n",
    "big_df = pd.concat([mainely_biz,pd.DataFrame({'status':status\n",
    "                                      ,'owner/org_type':org_type\n",
    "                                      ,'address':address\n",
    "                                      ,'owner_or_clerk':owner_clerk\n",
    "                                      ,'filing_date':filing_date})], axis=1)\n",
    "big_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_dupe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning\n",
    "\n",
    "List fields are converted to strings, preparing them for trimming and replacement of unneccessary characters. The script previews the dataframe again, to compare with the output from the previous step, before cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##DATA/STRING CLEANING\n",
    "\n",
    "#Convert lists to strings\n",
    "big_df[['address'\n",
    "        ,'owner_or_clerk'\n",
    "        ,'filing_date']] = big_df[['address'\n",
    "                                   ,'owner_or_clerk'\n",
    "                                   ,'filing_date']].astype(str)\n",
    "\n",
    "#Eliminate <td> tags\n",
    "replace_dict = {'<td>':''\n",
    "               ,'</td>':''\n",
    "               ,r'\\[|\\]':''\n",
    "               ,r'\\\\n|\\\\t':''\n",
    "               ,r\"\\'\":''\n",
    "               }\n",
    "\n",
    "big_df.replace(replace_dict,regex=True,inplace=True)\n",
    "\n",
    "#Cleaning HTML out of data\n",
    "def trim_all_columns(df):\n",
    "    \"\"\"\n",
    "    Trim whitespace from all series in dataframe\n",
    "    \"\"\"\n",
    "    trim_strings = lambda x: x.strip() if isinstance(x, str) else x\n",
    "    return df.applymap(trim_strings)\n",
    "\n",
    "big_df = trim_all_columns(big_df)\n",
    "\n",
    "#PREVIEW DATAFRAME\n",
    "big_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pull in list of registered agents\n",
    "#From: https://www5.informe.org/cgi-bin/online/moraa/cra_list.pl\n",
    "#Separates out addresses and picks only text\n",
    "\n",
    "sel = Selector(text = requests.get('https://www5.informe.org/cgi-bin/online/moraa/cra_list.pl').content)\n",
    "\n",
    "name=sel.xpath('//html//table[@class=\"at-data-table\"]//tr/td[1]/text()').getall()\n",
    "number=sel.xpath('//html//table[@class=\"at-data-table\"]//tr/td[2]/text()').getall()\n",
    "address1=sel.xpath('//html//table[@class=\"at-data-table\"]//tr/td[3]/text()[1]').getall()\n",
    "address2=sel.xpath('//html//table[@class=\"at-data-table\"]//tr/td[3]/text()[2]').getall()\n",
    "tel=sel.xpath('//html//table[@class=\"at-data-table\"]//tr/td[4]/text()').getall()\n",
    "email=sel.xpath('//html//table[@class=\"at-data-table\"]//tr/td[5]/text()').getall()\n",
    "\n",
    "agents = pd.DataFrame({'name':name\n",
    "              ,'number':number\n",
    "              ,'address1':address1\n",
    "              ,'address2':address2\n",
    "              ,'tel':tel\n",
    "              ,'email':email})\n",
    "\n",
    "agents.drop([0])\n",
    "\n",
    "#Concatenate full address\n",
    "agents['full_address'] = agents['address1'] + ' ' + agents['address2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## WRITE TO DATA.WORLD ##\n",
    "with dw.open_remote_file('darrenfishell/mainely-businesses', 'maine-registered-agents.csv') as w:\n",
    "    agents.to_csv(w, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OUTPUT TO CSV\n",
    "cwd = os.getcwd()\n",
    "big_df.to_csv('mainely_businesses_scraped.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## WRITE TO DATA.WORLD ##\n",
    "with dw.open_remote_file('darrenfishell/mainely-businesses', 'raw-mainely-business-names.csv') as w:\n",
    "    big_df.to_csv(w, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
